{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/S+i8mcuNXqDREPUa6Kzm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIDs7xacLJsQ",
        "outputId": "94a8135a-5168-43bd-806e-c560049a3d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renewable energy has become a pivotal topic in the global conversation about sustainable development and climate change. The shift towards renewable energy sources such as solar, wind, hydro, and geothermal power is increasingly seen as a crucial strategy for reducing greenhouse gas emissions and mitigating environmental damage. The intermittent nature of solar and wind energy requires the development of efficient energy storage solutions to ensure a stable power supply.\n"
          ]
        }
      ],
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# # Load the summarization pipeline\n",
        "# summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# # Sample text to summarize\n",
        "# text = \"\"\"\n",
        "# Hugging Face is a company specializing in natural language processing. They are best known for their work on transformer models and the open-source library, `transformers`, which provides pre-trained models for various NLP tasks. Their library is widely used in both academia and industry to tackle challenges in language understanding and generation.\n",
        "# \"\"\"\n",
        "\n",
        "# # Generate summary\n",
        "# summary = summarizer(text, max_length=50, min_length=25, do_sample=True)\n",
        "# print(summary[0]['summary_text'])\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a different summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Updated sample text to summarize\n",
        "text = \"\"\"\n",
        "Renewable energy has become a pivotal topic in the global conversation about sustainable development and climate change. As the world grapples with the consequences of fossil fuel consumption, the shift towards renewable energy sources such as solar, wind, hydro, and geothermal power is increasingly seen as a crucial strategy for reducing greenhouse gas emissions and mitigating environmental damage.\n",
        "\n",
        "Solar energy harnesses the power of the sun through photovoltaic cells, converting sunlight directly into electricity. This technology has seen remarkable advancements over the past decade, with decreasing costs and increasing efficiency making solar power a viable option for both residential and commercial applications. Wind energy, captured through wind turbines, also contributes significantly to the renewable energy mix. Onshore and offshore wind farms have proliferated, taking advantage of wind patterns to generate large amounts of electricity.\n",
        "\n",
        "Hydropower, which utilizes flowing water to generate energy, has been a longstanding source of renewable energy. While it provides a stable and consistent supply of electricity, large-scale hydroelectric projects can have significant environmental and social impacts, including effects on aquatic ecosystems and local communities. Geothermal energy, derived from the heat of the Earth’s interior, offers a reliable and sustainable source of power with relatively low environmental impact.\n",
        "\n",
        "Despite these advancements, the transition to renewable energy faces several challenges. The intermittent nature of solar and wind energy requires the development of efficient energy storage solutions to ensure a stable power supply. Additionally, the integration of renewable energy sources into existing power grids necessitates significant infrastructure upgrades and technological innovations.\n",
        "\n",
        "Policy and regulatory frameworks also play a critical role in supporting the growth of renewable energy. Governments around the world are implementing incentives and subsidies to encourage the adoption of clean energy technologies. However, the effectiveness of these measures varies by region and depends on local economic, political, and social factors.\n",
        "\n",
        "The shift towards renewable energy represents not only an opportunity to combat climate change but also a chance to drive economic growth and create jobs in emerging industries. As technology continues to evolve and costs decrease, the potential for renewable energy to meet a significant portion of global energy demands becomes increasingly achievable. However, achieving a sustainable energy future will require continued innovation, investment, and collaboration across all sectors of society.\n",
        "\"\"\"\n",
        "\n",
        "# Generate summary with adjusted parameters\n",
        "summary = summarizer(text, max_length=150, min_length=70, do_sample=True)\n",
        "print(summary[0]['summary_text'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfM6rKhvUzly",
        "outputId": "14e8b5be-957c-4514-c0b5-f2775f80538f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JepzakWbLLHv",
        "outputId": "0ed21abc-3780-4336-c088-629db7b9aa42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Example text\n",
        "text = \"Hello, world!\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors=\"tf\")\n",
        "\n",
        "# Perform inference\n",
        "outputs = model(inputs)\n",
        "\n",
        "# Extract logits\n",
        "logits = outputs.logits\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "# Print logits and probabilities\n",
        "print(\"Logits:\", logits)\n",
        "print(\"Probabilities:\", probabilities)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpNVWPb9mINg",
        "outputId": "4853fbe7-eaf6-4e86-f664-2a0f2773471b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tf.Tensor([[-0.03288737 -0.49167395]], shape=(1, 2), dtype=float32)\n",
            "Probabilities: tf.Tensor([[0.61272633 0.38727373]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOTcrC57qC_a",
        "outputId": "ed43712c-b1f2-40ec-f3bb-c50958330fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Example text\n",
        "text = \"Hello, world!\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract logits\n",
        "logits = outputs.logits\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "# Print logits and probabilities\n",
        "print(\"Logits:\", logits)\n",
        "print(\"Probabilities:\", probabilities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xE5NdPdqWH6",
        "outputId": "ffcef22a-8c89-4c42-fde4-c79f014f726f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[0.0392, 0.4516]])\n",
            "Probabilities: tensor([[0.3983, 0.6017]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Example text\n",
        "text = \"I hate this product.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Extract logits\n",
        "logits = outputs.logits\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "# Get predicted class\n",
        "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "# Define class labels\n",
        "class_labels = [\"Negative\",\"Positive\"]\n",
        "\n",
        "# Print results\n",
        "print(\"Logits:\", logits)\n",
        "print(\"Probabilities:\", probabilities)\n",
        "print(\"Predicted class:\", class_labels[predicted_class])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdcvjRagrEb9",
        "outputId": "8ac6a3d7-032a-4ed6-aa80-e2af3ef2d4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[-0.3235, -0.4736]])\n",
            "Probabilities: tensor([[0.5374, 0.4626]])\n",
            "Predicted class: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# import PyPDF2\n",
        "\n",
        "# # Load the summarization model\n",
        "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# # Extract text from PDF\n",
        "# def extract_text_from_pdf(pdf_path):\n",
        "#     text = \"\"\n",
        "#     with open(pdf_path, \"rb\") as file:\n",
        "#         reader = PyPDF2.PdfReader(file)\n",
        "#         for page in range(reader.numPages):\n",
        "#             text += reader.getPage(page).extract_text()\n",
        "#     return text\n",
        "\n",
        "# # Path to your PDF\n",
        "# pdf_path = f\"/content/LAST HALF (1)_merged (1).pdf\"\n",
        "# text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# # Generate summary\n",
        "# summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "# print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "id": "aIcN1O_0wsYg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "3b183f4b-e4d4-486c-b12f-1fb0b61ded6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DeprecationError",
          "evalue": "reader.numPages is deprecated and was removed in PyPDF2 3.0.0. Use len(reader.pages) instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDeprecationError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-ccda7336e878>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Path to your PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpdf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/LAST HALF (1)_merged (1).pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Generate summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-ccda7336e878>\u001b[0m in \u001b[0;36mextract_text_from_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumPages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PyPDF2/_reader.py\u001b[0m in \u001b[0;36mnumPages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mUse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mdeprecation_with_replacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reader.numPages\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"len(reader.pages)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3.0.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_num_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PyPDF2/_utils.py\u001b[0m in \u001b[0;36mdeprecation_with_replacement\u001b[0;34m(old_name, new_name, removed_in)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0mRaise\u001b[0m \u001b[0man\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mthat\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mwas\u001b[0m \u001b[0malready\u001b[0m \u001b[0mremoved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mhas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \"\"\"\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0mdeprecation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEPR_MSG_HAPPENED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PyPDF2/_utils.py\u001b[0m in \u001b[0;36mdeprecation\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mDeprecationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDeprecationError\u001b[0m: reader.numPages is deprecated and was removed in PyPDF2 3.0.0. Use len(reader.pages) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "import textwrap\n",
        "\n",
        "# Load the summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in range(len(reader.pages)):\n",
        "            text += reader.pages[page].extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to summarize text in chunks\n",
        "def summarize_text(text, chunk_size=1000):\n",
        "    chunks = textwrap.wrap(text, chunk_size)\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=150, min_length=30, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "    return \" \".join(summaries)\n",
        "\n",
        "# Path to your PDF\n",
        "pdf_path = \"/content/LAST HALF (1)_merged (1).pdf\"\n",
        "\n",
        "# Extract text from the PDF file\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Generate summary\n",
        "summary = summarize_text(text)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW3SLj-aDM20",
        "outputId": "e145fb74-cb2c-4333-c56a-f1e1e28325f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 150, but your input_length is only 106. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mini Project (22IS49) report   “AI Powered  Credit  Card  Fraud  Detection  System”   BACHELOR  OF ENGINEERING   In  INFORMATION SCIENCE  AND  ENGINEerING. Submitted by    Shravya  H Jain (1DS22IS140)  Shreya  M (1 DS22IS148) Shweta Kumari  (1ds22IS153) Sneha Pal  ( 1DS 22IS160) Under the Guidance  of  Dr. Chandrakala  B.M  Assistant  Professor,  Dept. of ISE   2023 -2024. The Mini Project report has been approved as it satisfies the academic requir ements prescribed for the Bachelor  of Engineering degree. It is a great pleasure for us  to acknowledge the assistance and support of a large number of  individuals who have been  responsible for the successful  completion of this project. We take this opportunity to express our sincere gratitude to Dayananda Sagar College of  Engineering for having  provided us with  a great opportunity  to pursue our Bachelor Degree. We would like to thank Dr. B G Prasad, Principal, Dayananda Sagar College of  Engineering for his constant  encouragement and advice. Special thanks to Dr. Annapurna P Patil, Professor and HOD, Department of Information  Science & Engineering. We are highly indebted to our internal guide Dr. Chandrakala B.M. for their constant support and guidance.  cashless transactions has led to a surge in credit card usage. Detecting and pre venting credit card fraud have thus become critical challenges. This paper provides a comprehensive review and implementation of  various machine learning algorithms to detect credit card Fraud. Ensemble classifiers with boosting techniques, such as AdaBoost, Gradient Boosting,  and XGBoost, combine predictions from several base estimators to improve performance. An extensive review of existing and proposed models for credit card fraud detection is conducted, followed by a comparative study of these techniques. Fraud detection systems must leverage advanced machine learning techniques to develop robust fraud detection systems. These systems must be able to protect financial institutions and their customers from substantial losses. Credit card fraud is a growing concern in the financial industry. As digital transactions become increasingly prevalent, the sophistication and frequency of fraudulent activities have escalated. Traditional methods of fraud detection are no longer sufficient to combat the ever -evolving tactics employed by fraudsters. The primary objective is to develop and implement an accurate and efficient machine learning model capable of detecting fraudulent credit card transactions in real -time. This objective encompasses the following  specific goals:   1. Achieve  a high level of accuracy in distinguishing  between  fraudulent  and  legitimate  transactions. 2. Minimize  financial  losses : Reduce  the financial  impact of fraudulent  activities on both  credit card issuers and cardholders. 3. Enhance  the security  of credit  card transactions, thereby  increasing  user trust and confidence. 4. Develop  a model that can adapt  to new and evolving  fraud patterns through continuous  learning and updates. 5. Ensure  the solution is computationally  The system will provide robust protection against fraudulent activities, safeguarding financial interests and enhancing the overall security of credit card usage. Maintain  compliance  with relevant financial  regulations and data protection laws to ensure  data privacy and security. AI Powered Credit Card Fraud Detection is a project of ISE,DSCE. The project aims to reduce financial losses for both consumers and financial institutions. It also aims to increase consumer confidence in the security of credit card transactions. Fraud detection systems need to be able to adapt to evolving fraud tactics. They must also comply with financial regulations and data protection laws. Fraud detection systems must also be capable of handling increasing transaction volumes without compromising performance. The project aims to create a robust and effective fraud detection system that not only safeguards financial assets but also enhances the overall trust and security of credit card transactions. Each method's advantages and disadvantages are discussed, highlighting trade -offs between accuracy, interpretability, scalability, and computational  requirements. DTs are praised for their interpretability but are prone to overfitting, while NNs excel at detecting intricate patterns but demand signi ficant  computational  resources. In the present world, we are facing a lot of  credit card problems. To detect the fraudulent activities the credit card fraud detection  system was  introduced. This proj ect aims  to focus  mainly  on machine learning  algorithms. The algorithms  used  are random forest algorithm and the Adaboost algorithm. The increase of credit card fraud is growing rapidly worldwide, which is the reason  actions should be taken to stop fraudsters. Detection of the fraudulent transactions wil l be made by using three machine  learning techniques KNN, SVM and Logistic Regression. Those models will be used on a credit   card transaction  dataset.  existing fraud detection systems rely on predefined rules created by domain experts. These rules are based on known patterns of fraudulent behavior and include criteria such as transaction amount limits, geographic locatio n, and purchase frequency. The proposed system aims to leverage advanced machine learning techniques to create a robust,  credit card fraud detection system. The proposed system would use logistic regression, decision trees, and clustering methods.  system that addresses the limitations of existing systems. The system will focus on accuracy, efficiency, adaptability, and scalability. It will ensure compliance with relevant regulations and maintaining data privacy.  Hyperparameter Tuning: Optimize the performance of the chosen models through systematic hyperparameter tuning. Anomaly Detection: Explore techniques such as Isolation Forest and One -Class SVM to detect anomalies that may indicate fraudulent activity. Scalability: Design th e system to handle increasing transaction volumes without increasing costs. The proposed credit card fraud detection system architecture is designed to be modular, scalable, and efficient. It incorporates various components for data collection, processing, machine learning,  and real-time fraud detection. Data Cleaning and Preprocessing: Implement ETL (Extract, Transform, Load) processes to clean and preprocess data. Data Ingestion: Use tools like Apache Kafka or AWS Kinesis for real -time data streaming and ingestion. Data Storage: Store processed data in scalable and secure storage solutions such as  Amazon S3, HDFS, or a relational database (e.g., Postgres) Machine Learning: Train models and train data. Assess models using metrics such as precision, recall, F1 -score, and ROC -AUC. Experiment with different algorithms such as gradient boosting, random forests, and neural networks. Optimize model  performance using techniques like Grid Search or Random  Search. The architecture is modular, scalable, and efficient. It provides a robust solution for real-time credit card fraud detection. It uses tools like AWS SageMaker, TensorFlow and Docker containers. Use popular machine learning frameworks such as TensorFlow, PyTorch, scikit -learn. Data processing libraries like Pandas and NumPy are essential for data preprocessing  and manipulation tasks. Streamlit is an open -source Python library used for creating web applications for data science and machine learning projects. It allows developers to build interactive and data -driven apps with minimal effort. Kaggle API: For downloading datasets from Kaggle. GitHub/GitLab: Platforms for hosting repositories and project management. Hardware: CPU, RAM, storage. A robust network infrastructure is necessary for real -time transaction processing and communication between different components of the system. This includes   switches, routers, and potentially load balancers depending on the deployment architecture. for faster data access and processing. The datasets contain transactions made by credit cards in September 2013 by European cardholders. There are a total 284,807   transactions. It contains only numeric input variables which are the resul t of a PCA transformation. We have 492 frauds out of 284,807 transactions i.e., 0.172% transactions are fraudulent. The dataset is highly  unbalanced. A highly unbalanced dataset is one where the classes are not represented equally. This is common in many real -world scenarios, such as fraud detection. Clean and preprocess the data to handle missing  values,  outliers,  and inconsistencies. Check  for null values in the dataset. Handling imbalanced datasets, particularly in credit card fraud detection, is crucial for building an effective model. The preprocessed data is split into training and testing datasets in the 80:20 ratio to avoid  the problems of overfitting  and underfitting. We will be dividing the dataset into two main groups. One for training the model and the other for testing our trained model’s performance. Logistic Regression is a popular statistical method used for binary  classification problems. Decision Tree Classifier Algorithm is an algorithm that uses a tree -like  graph or model of decisions and their possible outcomes to predict the final decision. We  give label  to a new transaction that   AI Powered  Credit  Card  Fraud  Detection. Decision rules determine the outcome o f the content of  leaf node. Random Forest algorithm is a powerful tree learning technique in Machine Learning. It works by creating a number of Decision Trees during the training  phase. Each tree is constructed using a random subset of the data set to measure a random  subset of features in each pa rtition. Streamlite is an open-source framework for creating web applications for machine learning and data science projects. It allows developers to build interactive and data-driven web apps quickly and with minimal coding. In the context of  credit card fraud detection, Streamlit can be used to create a user -friendly interface for monitoring transactions. Building the front end for a credit card fraud detection system using Streamlit involves several steps. App creation: Create a Python script for the Streamlit app. Deployment:Deploy the app to a web server or Streamlit  Sharing for broader access. Developing a credit card fault detection project involves a multifaceted approach that integrates advanced software capabilities with robust hardware infrastructure. The system must efficiently gather transaction data from  various sources, such as banks and payment gateways, ensuring seamless integration to maintain data integrity and consistency. Machine learning models for anomaly detection, real -time processing capabilities, and comprehensive security measures to protect sensitive information are essential. Organizations can build a resilient system capable of detecting and mitigating fraudulent activities, thereby safeguarding financial assets and customer trust. Unsupervised learning techniques to identify unusual patterns that don't  fit the established  norms of legitimate transactions. Multisource Data Integration: Combining data from various sources such as transaction history, social media, and device information. Big Data Analytics: Leveraging big data technologies to handle vast amounts of transaction data. Personalized Risk Scoring: Developing individualized risk scores for users based on their transaction  history and behavior. Machine Learning Group — ULB, Credit Card Fraud  Detection (2018), Kaggle. “Credit Card Fraud Detection: A Novel Approach Using Aggregation Strategy and Feedback Mechanism.” IEEE Internet of Things Journal 5 (2018): 3637 -3647. \"A Machine Learning  Approach for Detection of Fraud based on SVM\", International Journal of Scientific  Engineering  and Technology (2012) S. Maes, K. Tuyls, B. Vanschoenwinkel and B. Manderick. \"Credit card fraud detection   using Bayesian and neural  networks\", Proceedings of the 1st international naiso congresson  neuro  fuzzy technologies,  pp. 261 -270, 2017.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# import PyPDF2\n",
        "\n",
        "# # Load the summarization model\n",
        "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# # Extract text from PDF\n",
        "# def extract_text_from_pdf(pdf_path):\n",
        "#     text = \"\"\n",
        "#     with open(pdf_path, \"rb\") as file:\n",
        "#         reader = PyPDF2.PdfFileReader(file)\n",
        "#         for page in range(reader.numPages):\n",
        "#             text += reader.getPage(page).extract_text()\n",
        "#     return text\n",
        "\n",
        "# # Path to your PDF\n",
        "# pdf_path = f\"/content/{javalaboratory(1).pdf}\"  # Make sure this file is in your working directory\n",
        "# text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# # Generate summary\n",
        "# summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "# print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Xx3RE6cvx4ue",
        "outputId": "2e29ceca-a4f8-4175-982d-e2afd737b9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'javalaboratory' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-221d9c334053>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Path to your PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpdf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/{javalaboratory(1).pdf}\"\u001b[0m  \u001b[0;31m# Make sure this file is in your working directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'javalaboratory' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# import PyPDF2\n",
        "# from google.colab import files\n",
        "\n",
        "# # Load the summarization model\n",
        "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# # Function to extract text from PDF\n",
        "# def extract_text_from_pdf(pdf_path):\n",
        "#     text = \"\"\n",
        "#     with open(pdf_path, \"rb\") as file:\n",
        "#         reader = PyPDF2.PdfReader(file)\n",
        "#         for page in range(reader.len(reader.pages)):\n",
        "#             text += reader.getPage(page).extract_text()\n",
        "#     return text\n",
        "\n",
        "# # Upload the file\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Check if any files were uploaded\n",
        "# if not uploaded:\n",
        "#     print(\"No files uploaded.\")\n",
        "# else:\n",
        "#     # Save the uploaded file to the /content directory\n",
        "#     for filename in uploaded.keys():\n",
        "#         file_path = f\"/content/{filename}\"\n",
        "#         with open(file_path, \"wb\") as f:\n",
        "#             f.write(uploaded[filename])\n",
        "#         print(f\"File uploaded and saved to: {file_path}\")\n",
        "\n",
        "#     # Define the path to your PDF\n",
        "#     # Define the path to your PDF\n",
        "#     pdf_path = \"/content/Java Laboratory (1).pdf\"\n",
        "\n",
        "\n",
        "#     # Extract text from the PDF file\n",
        "#     try:\n",
        "#         text = extract_text_from_pdf(pdf_path)\n",
        "#         # Generate summary\n",
        "#         summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "#         print(summary[0]['summary_text'])\n",
        "#     except FileNotFoundError as e:\n",
        "#         print(f\"Error: {e}\")\n",
        "# from transformers import pipeline\n",
        "# import PyPDF2\n",
        "# from google.colab import files\n",
        "\n",
        "# # Load the summarization model\n",
        "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# # Function to extract text from PDF using PdfReader\n",
        "# def extract_text_from_pdf(pdf_path):\n",
        "#     text = \"\"\n",
        "#     with open(pdf_path, \"rb\") as file:\n",
        "#         reader = PyPDF2.PdfReader(file)\n",
        "#         for page in reader.pages:\n",
        "#             text += page.extract_text() or \"\"  # Ensure empty string if text extraction fails\n",
        "#     return text\n",
        "\n",
        "# # Upload the file\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Save the uploaded file to the /content directory\n",
        "# for filename in uploaded.keys():\n",
        "#     # Save the file to the path\n",
        "#     file_path = f\"/content/Java Laboratory (1).pdf\"\n",
        "#     with open(file_path, \"wb\") as f:\n",
        "#         f.write(uploaded[filename])\n",
        "\n",
        "# # Path to your PDF\n",
        "# pdf_path = file_path\n",
        "\n",
        "# # Extract text from the PDF file\n",
        "# text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# # Generate summary\n",
        "# summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "# print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Ez1K4A9-4ZgV",
        "outputId": "0cbb3c2f-93d0-423e-bdf5-03b53317f635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8d9d6deb-3db2-4347-b611-2865c165a12c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8d9d6deb-3db2-4347-b611-2865c165a12c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Java Laboratory (1).pdf to Java Laboratory (1) (1).pdf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-76f8728e9af6>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Generate summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         if (\n\u001b[1;32m    169\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             )\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         )\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   1697\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[1;32m    128\u001b[0m         ).expand(bsz, -1)\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "from google.colab import files\n",
        "import textwrap\n",
        "\n",
        "# Load the summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to summarize text in chunks\n",
        "def summarize_text(text, chunk_size=1000):\n",
        "    chunks = textwrap.wrap(text, chunk_size)\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=150, min_length=30, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "    return \" \".join(summaries)\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if any files were uploaded\n",
        "if not uploaded:\n",
        "    print(\"No files uploaded.\")\n",
        "else:\n",
        "    # Save the uploaded file to the /content directory\n",
        "    for filename in uploaded.keys():\n",
        "        file_path = f\"/content/Java Laboratory (1).pdf\"\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            f.write(uploaded[filename])\n",
        "        print(f\"File uploaded and saved to: {file_path}\")\n",
        "\n",
        "    # Define the path to your PDF\n",
        "    pdf_path = f\"/content/{filename}\"\n",
        "\n",
        "    # Extract text from the PDF file\n",
        "    try:\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        # Generate summary\n",
        "        summary = summarize_text(text)\n",
        "        print(summary)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "nn3W2mpg9ZsQ",
        "outputId": "fc339803-d80c-484e-dde8-a385f60b6f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d9a3eb42-c325-48fb-b221-802963486acc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d9a3eb42-c325-48fb-b221-802963486acc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Java Laboratory (1).pdf to Java Laboratory (1) (2).pdf\n",
            "File uploaded and saved to: /content/Java Laboratory (1).pdf\n",
            "The program creates a class Vehicle. The class should have two fields – no_of_seats, no-of-wheels. Create two objects – motorcycle and car for this class. Display the output to show the descriptions for car and motorcycle. System.out.println (\"Enter the name :\");   name=in.nextLine();   acc=Long. parseLong(in. nextLine());   bal= long.parseLong( in.next line);  System.out, \"--- Account Details --- \"; System. out.println(\"~~~~~~~~~~~~~~~~~\"), \"Name\": \"\", \"Account\", \"number\": \"1\", \"bal\": \"2\", \"name\": \"\"; \"display\": \"Account\" }); Design a super class called Employee with details as EmployeeId, name, Phone, and Salary. Extend this class by writing three subclasses namely Teaching, Technical, and Contract. Write a JAVA program to read and display at least 3 Employee objects of all three categories.  class Employee extends Employee. public void accept() { System.out.println(\"Enter Staff ID: \"); EmpID = obj.nextLine(); Empname =obj. nextLine(); System. out.println (\"Phone: \" + EmpPhone);    EmpSalary = obj,.nextFloat();  } public void display() {   System.in.show('Employee', Emp ID, Emp name, EmPPhone, Em pSalary); System.outed.println('Enter Phone number: \"), Emp phone. display('EmpPhone'); System.outs.on('Display', 'Employee,' 'Phone', 'EmpName', 'Emp public void accept() { super.accept(); Scanner obj = new Scanner(System.in); System.out.println(\"Enter number of Publications:\"); n = obj.nextInt(); n = \"Publications: \" + n);    }  } class Technical extends Employee  {   String skill;   public void display(){ super.display(); skill = \"Technical Skills\"; System. out.println('Enter Technical Skills:\"); skill.nextLine(); } } class Contract extends Employee extends Employee {    int period;  . public void accept(){ int domain = new int domain; System.Out.println (\"Doamin:\" + domain); . Public static void main(String[] args) { Teaching teach = new Teaching(); Contract con = new Contract(); System.out.println(\"Enter the details of Teaching Staff:\");   . super.display(); teach.accept(); con.accept() ; System. out.println (\"Contract Period: \" + period); Con.display() ; Con. display() ; Contract. display(); Con.Display() ; Enter Domain: Web Development. Enter number of Publications: 28. Enter name: Deepak Nayak. Enter Phone number: 8234156729. Enter Salary: 95532.56. Enter Technical Skills: Machine Learning, Python and Java.  raise an exception when b is equal to zero. Also demonstrate working of  ArrayIndexOutOfBoundException. when  b is not zero. Code:  import java.util.Scanner;   class ExceptionDemo  {  public static void main(String[]args) {  int a,b,result;  Scanner input =new Scanner(System.in);   System.out.println(\"Input two integers\"+array[5]);   try  { try   result=a/b; System. out.println (\"result = \"+result); } catch(ArithmeticException e) { System.in.print(e); } } URLMain.java is a java.net.URL class. It implements client-server communication, where client can send the message and server can receive the message without internet. Any give URL such as Protocol, file, port and host can be used.  java.net.*;  public class ServerSide   {   public static void main(String[]args)   try {  ServerSocket ss=new ServerSocket(3306);    Socket s=ss.accept(); //establishes connection   DataInputStream dis=new Data inputStream(s.getInputStream());   String str=(String)dis.readUTF(); System.out.println(\"message= \"+str); ss.close();   }  catch(Exception e) {  System. out.println(e); }  } //Part-2 ClientSide.java  import java.io.io;   importjava.net.net import java.sql;  import java.util;  public class InsertDetails { public static void main(String[]args) { System.out.println(\"Enter Student USN:\"); System. out.println (\"Enter Student Dept:\"); } try (Scanner obj, Statement stmt, Class.forName) { stmt.executeUpdate(q1); if(x>0) System.outs.status('Successfully Inserted', stmt); } catch (InterruptException) { console.error('interruptions' }); } } Enter Student Name:  Shreyas  Enter Student USN:  1DS22AI010. Enter Student Dept:  AIML. Successfully Inserted. Code:  //Part-2 Retrieve Details. Use JSP and JDBC connectivity ( login   authentication) using session. System.out.println(e);  Output:    1DS22IS073 Rithik ISE, 1 DS22EC044 Dhanya ECE, 1DD22AI010 Shreyas AIML. Program – 9 Structure the java servlet program to fetch the student details using JDBC. Code:  import java.io.IOException;  import javax.sql; import JDBC; import class. DriverManager.Connection con.getConnection(\"jdbc:mysql://localhost:3306/app \",\"root\",\"root\"); Statement st= con.createStatement(); ResultSet rs= st.executeQuery(\"select * from login where username='\"+ username+\"'\"); if(rs.getString(2).equals(pwd) { System.out.println(\"Invalid password try again\");        import javax.servlet.ServletException;   import jAVax.http.HttpServletRequest;  import java.io.PrintWriter. P9_DatabaseAccess is a public class that throws ServletException and IOException. \" + title + \"</h1>\\ n\" + \"#f0F0f0\\\">\\n\" +  \"center = \\\"center\\\">\" +title + \" </h1 >\"; out.println(\"<table border=1>\"); out. println(\"</body></html>\"); try (class.forName(\"com.mysql.jdbc.Driver\"), conn.getConnection(DB_URL, USER,  PASS), stmt.createStatement(); out.print(stmt.executeQuery(sql), conn.close); out.assert(conn.close()); out. print (stmt); }); out.write(stm.getString(3), Develop a Servlet program to demonstrate hit counter. Code:  import java.io.IOException;   import javax.servlet.http.ServletException. P10_1HitCounter extends HttpServlet. public void init() { private int hitcounter = 0; public void doGet(HttpServletRequest request, HhttpServletResponse response) throws ServletException, IOException. Response.setContentType(\"text/html\"); response.close(); String title = \"Total Number of Hits\"; String docType = \"<!doctype html public \\\"-//w3c//dtd html 4.0 \" + \"transitional//en\\\">\\ n\"; PrintWriter out = response.getWriter(); out.println(docType + \"<html>\\n\" + title + \",\" + hitcounter + \"\"; out.destroy()); out.print( \"\", out, \"\", \"\", true), out.write( \",\" \"\", false), \"\", { \"hitCount\" = hitcounter, \"title\" = \"\", bgcolor = \"#f0F0f0\", \"hitcounter\" = true, \"body\" =\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check the current working directory\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztkn72B22pOr",
        "outputId": "f675b53c-2906-4dff-9ed2-1038f1b211af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n",
            "['.config', 'Java Laboratory (1).pdf', 'LAST HALF (1)_merged (1).pdf', 'Java Laboratory (1) (2).pdf', '.ipynb_checkpoints', 'Java Laboratory (1) (1).pdf', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaY3BPZQx4xR",
        "outputId": "972db5e2-1948-4a12-e656-900a30e2ec80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    }
  ]
}